
% !TeX spellcheck = en_US
\documentclass[11pt]{article}
\usepackage{exerciseHead}

\input{Macros.tex} 

% Meta Information %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{semester_info.tex} % (file) for one update per semester

\title{\(L^2\) Hilbertspace}

%%%% Document %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\maketitle

\begin{exercise}[Hilbertspace]
	Prove that
	\[
		\langle X, Y \rangle := \E[XY]
	\]
	defines a scalar product on \(L^2\) (careful with the equivalence classes!).
	Deduce that the \(\real\)-vectorspace \(L^2\) becomes a Hilbertspace with
	this scalar product. What is the induced norm of the scalar product?
\end{exercise}

\begin{exercise}[Subspaces]
	Is the following space well defined?
	\[
		L_\real^2 := \{ [X]\in L^2 : X\equiv c \in \real \}
	\]
	What change needs to be made to make it well-defined? Prove that it is a
	complete linear subspace and therefore also a Hilbertspace.

	Do the same for
	\[
		L_0^2 := \{ [X]\in L^2 : \E[X] = 0\}.
	\]
	Show that these two linear subspaces are orthogonal in \(L^2\). Finally show,
	that
	\begin{equation}\label{eq: direct sum of linear subspaces}
		L^2 = L_\real^2 + L_0^2 = \{ X + Y : X\in L_\real^2, Y\in L_0^2\}
	\end{equation}
\end{exercise}

\begin{exercise}[Projections]
	Calculate and interpret the projection
	\[
		\pi_\real(Y):=\pi_{L_\real^2}(Y) = \argmin_{X\in L_\real^2} \| X-Y\|^2.
	\]	
	Do the same for \(\pi_0:=\pi_{L_0^2}\).
\end{exercise}
\begin{hint}
	Use the orthogonal decomposition \eqref{eq: direct sum of linear subspaces}.
\end{hint}

\begin{exercise}[Covariance]
	Prove that
	\[
		\Cov(X,Y) = \langle \pi_0(X), \pi_0(Y)\rangle.
	\]
	Deduce that the covariance function has to be bilinear but can not be positive
	definite.
\end{exercise}

\begin{exercise}[Linear Model]
	For the \(L^2\) subspace
	\[
		U = L_0^2 + \Span\{X_1,\dots, X_n\} = \Span\{\underbrace{X_0}_{:=1}, X_1,\dots, X_n\}
	\]
	calculate the projection
	\[
		\hat{Y} := \pi_U(Y) := \argmin_{\tilde{Y}\in U} \| \tilde{Y} - Y\|^2.
	\]
	You will encounter this linear model again in Stochastik 2.
\end{exercise}
\begin{hint}
	Use the parametrization \(\tilde{Y} = a^T X\) where \(X:=(X_0,\dots, X_n)^T\)
	and \(a\in \real^{n+1}\). Then justify
	\[
		0\overset!=\nabla_a \| a^T X - Y \|^2 = \E[\nabla_a (a^T X - Y )^2].
	\]
\end{hint}

\end{document}